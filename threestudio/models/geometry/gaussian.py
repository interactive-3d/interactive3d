#
# Copyright (C) 2023, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# This software is free for non-commercial, research and evaluation use 
# under the terms of the LICENSE.md file.
#
# For inquiries contact  george.drettakis@inria.fr
#
import torch
import numpy as np
from torch import nn
from typing import NamedTuple

import sys
from datetime import datetime
import random

from dataclasses import dataclass

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

import threestudio
from threestudio.models.geometry.base import BaseGeometry
from threestudio.utils.typing import *
from pathlib import Path
from threestudio.utils.mesh import load_mesh_as_pcd_trimesh

def get_qvec(cfg):
    qvec = torch.zeros(cfg.num_points, 4, dtype=torch.float32)
    qvec[:, 0] = 1.0
    return qvec

def get_svec(cfg):
    svec = torch.ones(cfg.num_points, 3, dtype=torch.float32) * cfg.svec_val
    return svec


def get_alpha(cfg):
    alpha = torch.ones(cfg.num_points, dtype=torch.float32) * cfg.alpha_val
    return alpha

def inverse_sigmoid(x):
    return torch.log(x/(1-x))

def get_expon_lr_func(
    lr_init, lr_final, lr_delay_steps=0, lr_delay_mult=1.0, max_steps=1000000
):
    """
    Copied from Plenoxels

    Continuous learning rate decay function. Adapted from JaxNeRF
    The returned rate is lr_init when step=0 and lr_final when step=max_steps, and
    is log-linearly interpolated elsewhere (equivalent to exponential decay).
    If lr_delay_steps>0 then the learning rate will be scaled by some smooth
    function of lr_delay_mult, such that the initial learning rate is
    lr_init*lr_delay_mult at the beginning of optimization but will be eased back
    to the normal learning rate when steps>lr_delay_steps.
    :param conf: config subtree 'lr' or similar
    :param max_steps: int, the number of steps during optimization.
    :return HoF which takes step as input
    """

    def helper(step):
        if step < 0 or (lr_init == 0.0 and lr_final == 0.0):
            # Disable this parameter
            return 0.0
        if lr_delay_steps > 0:
            # A kind of reverse cosine decay.
            delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(
                0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1)
            )
        else:
            delay_rate = 1.0
        t = np.clip(step / max_steps, 0, 1)
        log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)
        return delay_rate * log_lerp

    return helper

def strip_lowerdiag(L):
    uncertainty = torch.zeros((L.shape[0], 6), dtype=torch.float, device="cuda")

    uncertainty[:, 0] = L[:, 0, 0]
    uncertainty[:, 1] = L[:, 0, 1]
    uncertainty[:, 2] = L[:, 0, 2]
    uncertainty[:, 3] = L[:, 1, 1]
    uncertainty[:, 4] = L[:, 1, 2]
    uncertainty[:, 5] = L[:, 2, 2]
    return uncertainty

def strip_symmetric(sym):
    return strip_lowerdiag(sym)

def build_rotation(r):
    norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])

    q = r / norm[:, None]

    R = torch.zeros((q.size(0), 3, 3), device='cuda')

    r = q[:, 0]
    x = q[:, 1]
    y = q[:, 2]
    z = q[:, 3]

    R[:, 0, 0] = 1 - 2 * (y*y + z*z)
    R[:, 0, 1] = 2 * (x*y - r*z)
    R[:, 0, 2] = 2 * (x*z + r*y)
    R[:, 1, 0] = 2 * (x*y + r*z)
    R[:, 1, 1] = 1 - 2 * (x*x + z*z)
    R[:, 1, 2] = 2 * (y*z - r*x)
    R[:, 2, 0] = 2 * (x*z - r*y)
    R[:, 2, 1] = 2 * (y*z + r*x)
    R[:, 2, 2] = 1 - 2 * (x*x + y*y)
    return R

def build_scaling_rotation(s, r):
    L = torch.zeros((s.shape[0], 3, 3), dtype=torch.float, device="cuda")
    R = build_rotation(r)

    L[:,0,0] = s[:,0]
    L[:,1,1] = s[:,1]
    L[:,2,2] = s[:,2]

    L = R @ L
    return L

def safe_state(silent):
    old_f = sys.stdout
    class F:
        def __init__(self, silent):
            self.silent = silent

        def write(self, x):
            if not self.silent:
                if x.endswith("\n"):
                    old_f.write(x.replace("\n", " [{}]\n".format(str(datetime.now().strftime("%d/%m %H:%M:%S")))))
                else:
                    old_f.write(x)

        def flush(self):
            old_f.flush()

    sys.stdout = F(silent)

    random.seed(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.set_device(torch.device("cuda:0"))


class BasicPointCloud(NamedTuple):
    points : np.array
    colors : np.array
    normals : np.array


@threestudio.register("gaussian")
class GaussianModel(BaseGeometry):
    
    @dataclass
    class Config(BaseGeometry.Config):
        sh_degree: int = 0
        position_lr_init: float = 0.0001
        position_lr_final: float = 0.00001
        position_lr_delay_mult: float = 0.02
        position_lr_max_steps: int = 3000
        scale_lr_init: float = 0.003
        scale_lr_final: float = 0.001
        scale_lr_max_steps: int = 3000
        feature_lr: float = 0.01
        opacity_lr: float = 0.05
        scaling_lr: float = 0.005
        rotation_lr: float = 0.005
        densification_interval: int = 50
        prune_interval: int = 50
        opacity_reset_interval: int = 100000
        densify_from_iter: int = 100
        prune_from_iter: int = 100
        densify_until_iter: int = 2000
        prune_until_iter: int = 2000
        densify_grad_threshold: float = 0.01
        min_opac_prune: float = 0.05
        split_thresh: float = 0.02
        radii2d_thresh: float = 1000
        init_num_pts: int = 100
        pc_init_radius: float = 0.8
        opacity_init: float = 0.8
        scales_init: float = 0.02
        # init
        init: bool = False
        type: str = "mesh"
        mesh: str = "debug_data/sample_199.ply"
        rotate_xy: bool = False
        flip_z: bool = False
        flip_x: bool = False
        prompt: str = "a human face"
        num_points: int = 4096
        mean_std: float = 0.8
        svec_val: float = 0.02
        alpha_val: float = 0.8
        random_color: bool = True
        facex: bool = True

    cfg: Config

    def setup_functions(self):
        def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):
            L = build_scaling_rotation(scaling_modifier * scaling, rotation)
            actual_covariance = L @ L.transpose(1, 2)
            symm = strip_symmetric(actual_covariance)
            return symm
        
        self.scaling_activation = torch.exp
        self.scaling_inverse_activation = torch.log

        self.covariance_activation = build_covariance_from_scaling_rotation

        self.opacity_activation = torch.sigmoid
        self.features_activation = torch.sigmoid
        # self.features_activation = torch.nn.Identity()
        self.inverse_opacity_activation = inverse_sigmoid

        self.rotation_activation = torch.nn.functional.normalize


    def configure(self) -> None:
        super().configure()
        self.active_sh_degree = 0
        self.max_sh_degree = self.cfg.sh_degree  
        self._xyz = torch.empty(0)
        self._features = torch.empty(0)
        self._scaling = torch.empty(0)
        self._rotation = torch.empty(0)
        self._opacity = torch.empty(0)
        self.max_radii2D = torch.empty(0)
        self.xyz_gradient_accum = torch.empty(0)
        self.denom = torch.empty(0)
        self.optimizer = None
        self.setup_functions()
    
    def mesh_initlization(self):
        cfg = self.cfg
        assert cfg.type == 'mesh', "currently we only support mesh"
        mesh_path = Path(cfg.mesh)
        assert mesh_path.exists(), f"Mesh path {mesh_path} does not exist"
        xyz, rgb = load_mesh_as_pcd_trimesh(mesh_path, cfg.num_points)
        if rgb.shape[-1] == 4:
            rgb = rgb[..., :3]

        xyz -= xyz.mean(dim=0, keepdim=True)

        xyz = xyz / (xyz.norm(dim=-1).max() + 1e-5)
        xyz = xyz * cfg.mean_std

        # if xyz.shape[0] > cfg.num_points:
        #     _, idx = farthest_point_sampling(xyz, cfg.num_points)
        #     xyz = xyz[idx]
        #     rgb = rgb[idx]
        # else:
        #     cfg.num_points = xyz.shape[0]
        if cfg.get("rotate_xy", False):
            print("[red]will rotate the x and y axis by lihe")
            x, y, z = xyz.chunk(3, dim=-1)
            xyz = torch.cat([-y, x, z], dim=-1)

        if cfg.get("flip_yz", False):
            print("[red]will flip the y and z axis")
            x, y, z = xyz.chunk(3, dim=-1)
            xyz = torch.cat([x, z, y], dim=-1)
        
        if cfg.get("flip_z", False):
            print("[red]will flip the z axis")
            x, y, z = xyz.chunk(3, dim=-1)
            xyz = torch.cat([x, y, -z], dim=-1)
        
        if cfg.get("flip_x", False):
            print("[red]will flip the x axis")
            x, y, z = xyz.chunk(3, dim=-1)
            xyz = torch.cat([-x, y, z], dim=-1)

        if cfg.get("flip_xy", False):
            print("[red]will flip the x and y axis")
            x, y, z = xyz.chunk(3, dim=-1)
            xyz = torch.cat([y, x, z], dim=-1)

        if cfg.svec_val > 0.0:
            svec = get_svec(cfg)
        else:
            # svec = nearest_neighbor_initialize(xyz, k=3)[..., None].repeat(1, 3)
            raise NotImplementedError
        alpha = get_alpha(cfg)
        qvec = get_qvec(cfg)

        if cfg.get("random_color", True):
            rgb = torch.rand_like(rgb)

        # NOTE(lihe): save the init pcd
        print("====saving the processed init pcd from mesh====")
        np.save('debug_data/pcd_mesh.npy', xyz.detach().cpu().numpy())

        initial_values = {}
        initial_values["mean"] = xyz
        initial_values["color"] = rgb
        initial_values["svec"] = svec
        initial_values["qvec"] = qvec
        initial_values["alpha"] = alpha
        initial_values["raw"] = False

        return initial_values

    @property
    def get_scaling(self):
        return self.scaling_activation(self._scaling)
    
    @property
    def get_rotation(self):
        return self.rotation_activation(self._rotation)
    
    @property
    def get_xyz(self):
        return self._xyz
    
    @property
    def get_features(self):
        return self.features_activation(self._features)
    
    @property
    def get_opacity(self):
        return self.opacity_activation(self._opacity)
    
    def get_covariance(self, scaling_modifier = 1):
        return self.covariance_activation(self.get_scaling, scaling_modifier, self._rotation)

    def init_params(self,):
        if self.cfg.init:
            initial_values = self.mesh_initlization()
            if "raw" in initial_values:
                raw = initial_values["raw"]
            num_pts = self.cfg.num_points
            print(f"Initilizing point cloud ({num_pts}) from mesh...")
            self._xyz = nn.Parameter(initial_values["mean"])
            self._rotation = nn.Parameter(initial_values["qvec"])
            if not raw:
                self._scaling = nn.Parameter(
                    torch.log(initial_values["svec"])
                )
                self._features = nn.Parameter(
                    inverse_sigmoid(initial_values["color"]).unsqueeze(1)
                )
                self._opacity = nn.Parameter(
                    inverse_sigmoid(initial_values["alpha"]).unsqueeze(1)
                )
            self.max_radii2D = torch.zeros((self._xyz.shape[0]), device="cuda")
        else:
            # Since this data set has no colmap data, we start with random points
            num_pts = self.cfg.init_num_pts
            print(f"Generating random point cloud ({num_pts})...")
            phis = np.random.random((num_pts,)) * 2 * np.pi
            costheta = np.random.random((num_pts,)) * 2 - 1
            thetas = np.arccos(costheta)
            mu = np.random.random((num_pts,))
            radius = self.cfg.pc_init_radius * np.cbrt(mu)
            x = radius * np.sin(thetas) * np.cos(phis)
            y = radius * np.sin(thetas) * np.sin(phis)
            z = radius * np.cos(thetas)
            xyz = np.stack((x, y, z), axis=1)

            fused_point_cloud = torch.tensor(np.asarray(xyz)).float().cuda()
            features = torch.rand(num_pts, 3, 1).float().cuda() # bug here, should be passed to inverse function

            print("Number of points at initialisation : ", fused_point_cloud.shape[0])

            scales = torch.log(
                torch.ones(
                    num_pts, 3, 
                    dtype=torch.float32
                ) * self.cfg.scales_init
            )
            rots = torch.zeros((num_pts, 4), device="cuda")
            rots[:, 0] = 1

            opacities = inverse_sigmoid(
                self.cfg.opacity_init * torch.ones(
                    (fused_point_cloud.shape[0], 1), 
                    dtype=torch.float, device="cuda"
                )
            )

            self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))
            self._features = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))
            self._scaling = nn.Parameter(scales.requires_grad_(True))
            self._rotation = nn.Parameter(rots.requires_grad_(True))
            self._opacity = nn.Parameter(opacities.requires_grad_(True))
            self.max_radii2D = torch.zeros((self._xyz.shape[0]), device="cuda")
            
            self.fused_point_cloud = fused_point_cloud.cpu().clone().detach()
            self.features = features.cpu().clone().detach()
            self.scales = scales.cpu().clone().detach()
            self.rots = rots.cpu().clone().detach()
            self.opacities = opacities.cpu().clone().detach()

    def training_setup(self):
        training_args = self.cfg
        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")
        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")

        l = [
            {'params': [self._xyz], 'lr': training_args.position_lr_init, "name": "xyz"},
            {'params': [self._features], 'lr': training_args.feature_lr, "name": "features"},
            {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
            {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
            {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"}
        ]

        self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)
        self.xyz_scheduler_args = get_expon_lr_func(
            lr_init=training_args.position_lr_init,
            lr_final=training_args.position_lr_final,
            lr_delay_mult=training_args.position_lr_delay_mult,
            max_steps=training_args.position_lr_max_steps
        )
        
        self.scale_scheduler_args = get_expon_lr_func(
            lr_init=training_args.scale_lr_init,
            lr_final=training_args.scale_lr_final,
            lr_delay_mult=training_args.position_lr_delay_mult,
            max_steps=training_args.scale_lr_max_steps
        )

    def update_xyz_learning_rate(self, iteration):
        ''' Learning rate scheduling per step '''
        for param_group in self.optimizer.param_groups:
            if param_group["name"] == "xyz":
                lr = self.xyz_scheduler_args(iteration)
                param_group['lr'] = lr
                return lr
            if param_group["name"] == "scaling":
                lr = self.scale_scheduler_args(iteration)
                param_group['lr'] = lr
                return lr

    def update_scale_learning_rate(self, iteration):
        ''' Learning rate scheduling per step '''
        for param_group in self.optimizer.param_groups:
            if param_group["name"] == "scaling":
                lr = self.scale_scheduler_args(iteration)
                param_group['lr'] = lr
                return lr

    def construct_list_of_attributes(self):
        l = ['x', 'y', 'z', 'nx', 'ny', 'nz']
        # All channels except the 3 DC
        for i in range(self._features.shape[1]*self._features.shape[2]):
            l.append('features_{}'.format(i))
        l.append('opacity')
        for i in range(self._scaling.shape[1]):
            l.append('scale_{}'.format(i))
        for i in range(self._rotation.shape[1]):
            l.append('rot_{}'.format(i))
        return l

    def reset_opacity(self):
        # opacities_new = inverse_sigmoid(torch.min(self.get_opacity, torch.ones_like(self.get_opacity)*0.01))
        opacities_new = inverse_sigmoid(self.get_opacity * 0.9)
        optimizable_tensors = self.replace_tensor_to_optimizer(opacities_new, "opacity")
        self._opacity = optimizable_tensors["opacity"]
        
    def to(self, device='cpu'):
        self._xyz = self._xyz.to(device)
        self._features = self._features.to(device)
        self._opacity = self._opacity.to(device)
        self._scaling = self._scaling.to(device)
        self._rotation = self._rotation.to(device)

    def replace_tensor_to_optimizer(self, tensor, name):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            if group["name"] == name:
                stored_state = self.optimizer.state.get(group['params'][0], None)
                # import pdb; pdb.set_trace()
                stored_state["exp_avg"] = torch.zeros_like(tensor)
                stored_state["exp_avg_sq"] = torch.zeros_like(tensor)

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter(tensor.requires_grad_(True))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors

    def _prune_optimizer(self, mask):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            stored_state = self.optimizer.state.get(group['params'][0], None)
            if stored_state is not None:
                stored_state["exp_avg"] = stored_state["exp_avg"][mask]
                stored_state["exp_avg_sq"] = stored_state["exp_avg_sq"][mask]

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter((group["params"][0][mask].requires_grad_(True)))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
            else:
                group["params"][0] = nn.Parameter(group["params"][0][mask].requires_grad_(True))
                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors

    def prune_points(self, mask):
        valid_points_mask = ~mask
        optimizable_tensors = self._prune_optimizer(valid_points_mask)

        self._xyz = optimizable_tensors["xyz"]
        self._features = optimizable_tensors["features"]
        self._opacity = optimizable_tensors["opacity"]
        self._scaling = optimizable_tensors["scaling"]
        self._rotation = optimizable_tensors["rotation"]

        self.xyz_gradient_accum = self.xyz_gradient_accum[valid_points_mask]

        self.denom = self.denom[valid_points_mask]
        self.max_radii2D = self.max_radii2D[valid_points_mask]

    def cat_tensors_to_optimizer(self, tensors_dict):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            assert len(group["params"]) == 1
            extension_tensor = tensors_dict[group["name"]]
            stored_state = self.optimizer.state.get(group['params'][0], None)
            if stored_state is not None:

                stored_state["exp_avg"] = torch.cat((stored_state["exp_avg"], torch.zeros_like(extension_tensor)), dim=0)
                stored_state["exp_avg_sq"] = torch.cat((stored_state["exp_avg_sq"], torch.zeros_like(extension_tensor)), dim=0)

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter(torch.cat((group["params"][0], extension_tensor), dim=0).requires_grad_(True))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
            else:
                group["params"][0] = nn.Parameter(torch.cat((group["params"][0], extension_tensor), dim=0).requires_grad_(True))
                optimizable_tensors[group["name"]] = group["params"][0]

        return optimizable_tensors

    def densification_postfix(self, new_xyz, new_features, new_opacities, new_scaling, new_rotation):
        d = {"xyz": new_xyz,
        "features": new_features,
        "opacity": new_opacities,
        "scaling" : new_scaling,
        "rotation" : new_rotation}

        optimizable_tensors = self.cat_tensors_to_optimizer(d)
        self._xyz = optimizable_tensors["xyz"]
        self._features = optimizable_tensors["features"]
        self._opacity = optimizable_tensors["opacity"]
        self._scaling = optimizable_tensors["scaling"]
        self._rotation = optimizable_tensors["rotation"]

        self.xyz_gradient_accum = torch.zeros((self._xyz.shape[0], 1), device="cuda")
        self.denom = torch.zeros((self._xyz.shape[0], 1), device="cuda")
        self.max_radii2D = torch.zeros((self._xyz.shape[0]), device="cuda")

    def densify_and_split(self, grads, grad_threshold, N=2):
        n_init_points = self._xyz.shape[0]
        # Extract points that satisfy the gradient condition
        padded_grad = torch.zeros((n_init_points), device="cuda")
        padded_grad[:grads.shape[0]] = grads.squeeze()
        selected_pts_mask = torch.where(padded_grad >= grad_threshold, True, False)
        selected_pts_mask = torch.logical_and(
            selected_pts_mask,
            torch.max(self.get_scaling, dim=1).values > self.cfg.split_thresh
        )

        stds = self.get_scaling[selected_pts_mask].repeat(N,1)
        means =torch.zeros((stds.size(0), 3),device="cuda")
        samples = torch.normal(mean=means, std=stds)
        rots = build_rotation(self._rotation[selected_pts_mask]).repeat(N,1,1)
        new_xyz = torch.bmm(rots, samples.unsqueeze(-1)).squeeze(-1) + self._xyz[selected_pts_mask].repeat(N, 1)
        new_scaling = self.scaling_inverse_activation(self.get_scaling[selected_pts_mask].repeat(N,1) / (0.8*N))
        new_rotation = self._rotation[selected_pts_mask].repeat(N,1)
        new_features = self._features[selected_pts_mask].repeat(N,1,1)
        new_opacity = self._opacity[selected_pts_mask].repeat(N,1)

        self.densification_postfix(new_xyz, new_features, new_opacity, new_scaling, new_rotation)

        prune_filter = torch.cat((selected_pts_mask, torch.zeros(N * selected_pts_mask.sum(), device="cuda", dtype=bool)))
        self.prune_points(prune_filter)

    def densify_and_clone(self, grads, grad_threshold):
        # Extract points that satisfy the gradient condition
        selected_pts_mask = torch.where(torch.norm(grads, dim=-1) >= grad_threshold, True, False)
        selected_pts_mask = torch.logical_and(
            selected_pts_mask,
            torch.max(self.get_scaling, dim=1).values <= self.cfg.split_thresh
        )
        
        new_xyz = self._xyz[selected_pts_mask]
        new_features = self._features[selected_pts_mask]
        new_opacities = self._opacity[selected_pts_mask]
        new_scaling = self._scaling[selected_pts_mask]
        new_rotation = self._rotation[selected_pts_mask]

        self.densification_postfix(new_xyz, new_features, new_opacities, new_scaling, new_rotation)

    def densify(self, max_grad):
        grads = self.xyz_gradient_accum / self.denom
        grads[grads.isnan()] = 0.0

        self.densify_and_clone(grads, max_grad)
        self.densify_and_split(grads, max_grad)
        
    def prune(self, min_opacity, max_screen_size):
        prune_mask = (self.get_opacity < min_opacity).squeeze()
        if max_screen_size:
            big_points_vs = self.max_radii2D > max_screen_size
            prune_mask = torch.logical_or(prune_mask, big_points_vs)
        self.prune_points(prune_mask)

        torch.cuda.empty_cache()

    def add_densification_stats(self, viewspace_point_tensor, update_filter):
        self.xyz_gradient_accum[update_filter] += torch.norm(viewspace_point_tensor.grad[update_filter,:2], dim=-1, keepdim=True)
        self.denom[update_filter] += 1
      
    @torch.no_grad()  
    def update_states(
        self, 
        iteration, 
        visibility_filter,
        radii,
        viewspace_point_tensor,
    ):
        # Keep track of max radii in image-space for pruning
        # loop over batch
        bs = len(viewspace_point_tensor)
        for i in range(bs):
            radii_i = radii[i]
            visibility_filter_i = visibility_filter[i]
            viewspace_point_tensor_i = viewspace_point_tensor[i]
            self.max_radii2D[visibility_filter_i] = torch.max(self.max_radii2D[visibility_filter_i], radii_i[visibility_filter_i])
            
            self.add_densification_stats(
                viewspace_point_tensor_i, 
                visibility_filter_i
            )
        # Densification
        if (iteration < self.cfg.densify_until_iter):
                
            if iteration > self.cfg.densify_from_iter and iteration < self.cfg.densify_until_iter and iteration % self.cfg.densification_interval == 0:
                self.densify(self.cfg.densify_grad_threshold)
                
            if iteration > self.cfg.prune_from_iter and iteration < self.cfg.prune_until_iter and iteration % self.cfg.prune_interval == 0:
                self.prune(
                    self.cfg.min_opac_prune, 
                    self.cfg.radii2d_thresh
                )